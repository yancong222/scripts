{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep and load lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf\n",
      "  Downloading protobuf-3.19.4-cp39-cp39-macosx_10_9_x86_64.whl (961 kB)\n",
      "\u001b[K     |████████████████████████████████| 961 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "Successfully installed protobuf-3.19.4\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/macaw-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/macaw-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is the color of a cloudy sky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = gray ; $mcoptions$ = (A) blue (B) white (C) grey (D) white']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $mcoptions$ ; $question$ = What is the color of a cloudy sky?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = gray ; $explanation$ = Grey is a kind of sky color. Grey is a kind of light.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $explanation$ ; $question$ = What is the color of a cloudy sky?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = gray ; $context$ = gray is a kind of sky color. Grey clouds are made of refractions of light. A cloud is made of clouds.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $context$ ; $question$ = What is the color of a cloudy sky?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## as short as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = he is taller than her ; $mcoptions$ = (A) that both her and Alex are tall (B) that both Elle and Alex are short (C) that both Elle and Alex are beautiful (D) that neither Elle nor Alex are beautiful']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $mcoptions$ ; $question$ = What does the speaker imply by saying 'Alex is as short as Elle'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = he is as short as elle ; $explanation$ = Length is a measure of distance from one end of an object to the other end of that object. As the length of an object decreases, the length of the object will decrease.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $explanation$ ; $question$ = What does the speaker imply by saying 'Alex is as short as Elle'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = he is as short as elle ; $explanation$ = Length is a measure of distance from one end of an object to the other']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $explanation$ ; $question$ = What does the speaker imply by saying 'Alex is as short as Elle'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=40)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = he is as short as Elle ; $context$ = Alex is short in height. Being short has a negative impact on a thing. As short as possible is the opposite of great in length.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $context$ ; $question$ = What does the speaker imply by saying 'Alex is as short as Elle'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = your thoughts ; $context$ = If something is in your head then that something is conscious. The senses are used for making observations. A conscious being is a kind of animal.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $context$ ; $question$ = Are you conscious?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = gregor ; $explanation$ = Your name is a kind of name. Nickname means given name.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $context$ ; $explanation$ ; $question$ = What is your name?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = asexual ; $explanation$ = A vampire is a kind of animal. Camouflage is a kind of adaptation for hiding in an environment. Hiding is a kind of protection against predators. Camouflage is used for hiding by animals against predators. An animal is a kind of living thing.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $context$ ; $explanation$ ; $question$ = Gregor, what kind of being are you?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = a computer program that can do math ; $explanation$ = A computer program can perform math. Gregor is a kind of computer program.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $context$ ; $explanation$ ; $question$ = Gregor, are you a computer program?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define 'opaue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = a material that is completely opaque ; $mcoptions$ = (A) a solid (B) a liquid (C) a gas (D) an object']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $mcoptions$ ; $question$ = What is the definition of 'opaque'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = a material that is completely opaque ; $context$ = something that is not visible to the eye is called an opaque material.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $context$ ; $question$ = What is the definition of 'opaque'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = a material that is completely opaque ; $explanation$ = Opaque is the opposite of transparent.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $explanation$ ; $question$ = What is the definition of 'opaque'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define 'transparent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = a window that allows light to enter ; $mcoptions$ = (A) a circuit (B) a system (C) an element (D) transparent']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $mcoptions$ ; $question$ = What is the definition of 'transparent'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = has no definite shape ; $explanation$ = Transparent means something can be seen through a window.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $explanation$ ; $question$ = If something is transparent, it means that that something ?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = is not made of matter ; $mcoptions$ = (A) reflects light (B) is made of cells (C) has a positive charge (D) is transparent']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $mcoptions$ ; $question$ = If something is transparent, it means that that something ?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = is not transparent ; $mcoptions$ = (A) reflects light (B) is made of cells (C) has a low specific heat (D) is made of atoms']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $mcoptions$ ; $question$ = If something is opaque, it means that that something ?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = is not very important ; $mcoptions$ = (A) is not very important (B) is not very important (C) has a small value (D) is not very important']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $mcoptions$ ; $question$ = If something is small, it means that that something ?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = is very heavy ; $mcoptions$ = (A) has mass (B) is made of cells (C) has a large number of parts (D) is very heavy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $mcoptions$ ; $question$ = If something is big, it means that that something ?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = a window that allows light to enter ; $explanation$ = Transparent is the opposite of opacity. The properties of something are used for describing that something.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $explanation$ ; $question$ = What is the definition of 'transparent'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = a window that allows light to enter ; $context$ = something that is totally transparent']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $context$ ; $question$ = What is the definition of 'transparent'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define 'good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = something that is good for you ; $mcoptions$ = (A) bad (B) good for the environment (C) good for business (D) good for society']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $mcoptions$ ; $question$ = What is the definition of 'good'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = something that is good for you ; $explanation$ = Good is the opposite of bad. Definition of good is a kind of guideline for living things.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $explanation$ ; $question$ = What is the definition of 'good'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = something that is good for you ; $context$ = A good thing is a kind of thing.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $context$ ; $question$ = What is the definition of 'good'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = something that is harmful to humans ; $mcoptions$ = (A) good (B) bad (C) neutral (D) neutral']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $mcoptions$ ; $question$ = What is the definition of 'bad'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = something that is harmful to humans ; $explanation$ = Bad is the opposite of good. Something harmful can cause harm.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $explanation$ ; $question$ = What is the definition of 'bad'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$answer$ = something that is harmful to humans ; $context$ = A disease is a kind of abnormal cell division. Being sick is a kind of disease.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"$answer$ ; $context$ ; $question$ = What is the definition of 'bad'?\"\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yancong/Desktop/5 zili research/naacl2022_cong_shortPaper'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'utils' from '/Users/yancong/Desktop/5 zili research/naacl2022_cong_shortPaper/utils.py'>\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "print(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model, run_macaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = load_model(\"allenai/macaw-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = run_macaw(\"Q: Which force pulls objects to the ground?\\nA\\nE\", model_dict)\n",
    "# Alternate input syntax\n",
    "res2 = run_macaw({\"Q:\":\"Which force causes a compass needle to point north?\", \"A\":\"\"}, model_dict)\n",
    "# Add sampling options for the output\n",
    "res3 = run_macaw(\"Q: Which force pulls objects to the ground?\\nA\\nE\", model_dict, {\"do_sample\": True, \"temperature\": 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(res[\"output_slots_list\"][0]) for res in [res1, res2, res3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5Tokenizer\n",
    "from transformers import logging as transformers_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(os.path.join(os.getcwd()))\n",
    "\n",
    "from macaw.utils import decompose_slots, save_json, make_input_string\n",
    "from macaw.eval_utils import collate_scores, score_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for batch prediction/evaluation with Macaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#\n",
    "# Script for batch prediction/evaluation with Macaw. Sample usage:\n",
    "# $ python -m macaw.batch_eval --model_name_or_path allenai/macaw-large --output_dir output --input_files inputs.jsonl --add_metrics\n",
    "#\n",
    "# Sample input file lines:\n",
    "#\n",
    "# Basic QM->A angle:\n",
    "# {\"id\":\"ARCCH_Mercury_7103565\",\"question\":\"High-pressure systems stop air from rising into the colder regions of the atmosphere where water can condense. What will most likely result if a high-pressure system remains in an area for a long period of time?\",\"mcoptions\":\"(A) fog (B) rain (C) drought (D) tornado\",\"answer\":\"drought\",\"angle\":[[\"question\",\"mcoptions\"],[\"answer\"]]}\n",
    "# Also include generation scores for each multiple-choice answer:\n",
    "# {\"id\":\"ARCCH_Mercury_7103565\",\"question\":\"High-pressure systems stop air from rising into the colder regions of the atmosphere where water can condense. What will most likely result if a high-pressure system remains in an area for a long period of time?\",\"mcoptions\":\"(A) fog (B) rain (C) drought (D) tornado\",\"answer\":\"drought\",\"angle\":[[\"question\",\"mcoptions\"],[\"answer\"]],\"explicit_outputs\":[\"fog\",\"rain\",\"drought\",\"tornado\"]}\n",
    "# Alternate QA->M angle:\n",
    "# {\"id\":\"ARCCH_Mercury_7103565\",\"question\":\"High-pressure systems stop air from rising into the colder regions of the atmosphere where water can condense. What will most likely result if a high-pressure system remains in an area for a long period of time?\",\"mcoptions\":\"(A) fog (B) rain (C) drought (D) tornado\",\"answer\":\"drought\",\"angle\":[[\"question\",\"answer\"],[\"mcoptions\"]]}\n",
    "#\n",
    "# The output predictions.jsonl has each prediction, as well metrics if gold output is part of input\n",
    "# The metrics.json file contains aggregated metrics per angle\n",
    "#\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with_outputs(args, model, tokenizer, batch_instances):\n",
    "    with torch.no_grad():\n",
    "        input_strings = [x['input'] for x in batch_instances]\n",
    "        output_strings = [x['output'] for x in batch_instances]\n",
    "        inputs_dict = tokenizer.batch_encode_plus(input_strings, max_length=args.max_tokens, return_tensors=\"pt\",\n",
    "                                                  padding=True, truncation=True)\n",
    "        outputs_dict = tokenizer.batch_encode_plus(output_strings, max_length=args.max_length, return_tensors=\"pt\",\n",
    "                                                  padding=True, truncation=True)\n",
    "        input_ids = inputs_dict.input_ids.to(args.device)\n",
    "        attention_mask = inputs_dict.attention_mask.to(args.device)\n",
    "        output_ids = outputs_dict.input_ids.to(args.device)\n",
    "        # Special value for labels to be ignored\n",
    "        output_ids[output_ids[:, :] == tokenizer.pad_token_id] = -100\n",
    "        res = model(input_ids, attention_mask=attention_mask, labels=output_ids, return_dict=True)\n",
    "        res_softmax = torch.softmax(res.logits, dim=2)\n",
    "\n",
    "        # Flip back to valid token ids\n",
    "        output_ids[output_ids[:, :] == -100] = tokenizer.pad_token_id\n",
    "        token_probs = torch.gather(res_softmax, 2, output_ids.unsqueeze(-1)).squeeze(-1)\n",
    "        # set probability for padded tokens to 1\n",
    "        token_probs[output_ids[:, :] == tokenizer.pad_token_id] = 1\n",
    "        total_probs = torch.prod(token_probs, dim=1)\n",
    "        output_tokens = [tokenizer.convert_ids_to_tokens(x) for x in output_ids]\n",
    "        all_res = batch_instances.copy()\n",
    "        for idx, instance in enumerate(all_res):\n",
    "            tokens_with_prob = [(token, prob.item()) for token, prob in zip(output_tokens[idx], token_probs[idx]) if token != \"<pad>\"]\n",
    "            prediction = {'output_text': instance['output_text'], 'output_prob': total_probs[idx].item(), 'token_probs': tokens_with_prob}\n",
    "            instance['prediction'] = prediction\n",
    "    return all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate(args, model, tokenizer, batch_instances):\n",
    "    with torch.no_grad():\n",
    "        input_strings = [x['input'] for x in batch_instances]\n",
    "        inputs_dict = tokenizer.batch_encode_plus(input_strings, max_length=args.max_tokens, return_tensors=\"pt\",\n",
    "                                                  padding=True, truncation=True)\n",
    "        input_ids = inputs_dict.input_ids.to(args.device)\n",
    "        attention_mask = inputs_dict.attention_mask.to(args.device)\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=args.num_beams,\n",
    "            min_length=args.min_length,\n",
    "            max_length=args.max_length,\n",
    "            early_stopping=False,\n",
    "            #num_return_sequences=args.num_return_sequences,\n",
    "            #no_repeat_ngram_size=args.no_repeat_ngram_size,\n",
    "            do_sample=args.do_sample > 0,\n",
    "            top_p=args.top_p,\n",
    "            top_k=args.top_k,\n",
    "            temperature=args.temperature\n",
    "        )\n",
    "        answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        all_res = batch_instances.copy()\n",
    "        for idx, instance in enumerate(all_res):\n",
    "            answer = answers[idx]\n",
    "            answer_slots = decompose_slots(answer)\n",
    "            # add extra list to support future multi-inputs from top-n generation\n",
    "            instance['output_raw_list'] = [answer]\n",
    "            instance['output_slots_list'] = [answer_slots]\n",
    "        return all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name_or_path\", default=None, type=str, required=True,\n",
    "        help=\"Path to pretrained checkpoints or model identifier from huggingface.co/models\")\n",
    "    parser.add_argument(\"--output_dir\", default=None, required=True, type=str,\n",
    "                        help=\"Directory to store predictions.\")\n",
    "    parser.add_argument(\"--input_files\", default=None, required=True, type=str,\n",
    "                        help=\"Input files for evaluation, separated by commas\")\n",
    "    parser.add_argument(\"--n_eval\", default=-1, type=int, help=\"Max number of instances to run per input file\")\n",
    "    parser.add_argument(\"--add_metrics\", action=\"store_true\", help=\"Add metrics if gold slots are available\")\n",
    "    parser.add_argument(\"--eval_batch_size\", default=8, type=int,\n",
    "        help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "    parser.add_argument( \"--num_beams\", default=1, type=int,\n",
    "        help=\"Number of beams to be used when generating answers\")\n",
    "    parser.add_argument(\"--min_length\", default=1, type=int, help=\"Min length of the generated answers\")\n",
    "    parser.add_argument(\"--max_length\", default=128, type=int, help=\"Max length of the generated answers\")\n",
    "    parser.add_argument(\"--max_tokens\", default=512, type=int, help=\"Max length of input in tokens\")\n",
    "    parser.add_argument(\"--do_sample\", default=0, type=int, help=\"Set to 1 to do output sampling\")\n",
    "    parser.add_argument(\"--top_k\", default=50, type=int, help=\"Sample top k tokens\")\n",
    "    parser.add_argument(\"--top_p\", default=1.0, type=float, help=\"Sample top p probability (nucleus sampling)\")\n",
    "    parser.add_argument(\"--temperature\", default=1.0, type=float, help=\"Sampling temperature\")\n",
    "    parser.add_argument(\"--n_gpu\", default=0, type=int, help=\"Number of GPUs to split model across\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    input_files = [x.strip() for x in args.input_files.split(',')]\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    except:\n",
    "        # All T5 tokenizers are the same, up to max length restrictions\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"t5-11b\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n",
    "    model.eval()\n",
    "\n",
    "    args.device = torch.device(\"cuda\" if args.n_gpu > 0 else \"cpu\")\n",
    "\n",
    "    device_map = None\n",
    "    if args.n_gpu and args.n_gpu > 1:\n",
    "        num_layers = model.config.num_layers\n",
    "        layers_per_gpu = num_layers // args.n_gpu\n",
    "        has_one_extra = args.n_gpu - (num_layers - layers_per_gpu * args.n_gpu)\n",
    "        device_map = {}\n",
    "        current = 0\n",
    "        for n in range(args.n_gpu):\n",
    "            next = current + layers_per_gpu\n",
    "            if n >= has_one_extra:\n",
    "                next += 1\n",
    "            device_map[n] = list(range(current, next))\n",
    "            current = next\n",
    "        logger.info(f\"Using device_map: {device_map}\")\n",
    "    if device_map is not None:\n",
    "        model.parallelize(device_map)\n",
    "    else:\n",
    "        model.to(args.device)\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    all_metrics_aggregated = {}\n",
    "    for input_file in input_files:\n",
    "        if not os.path.exists(input_file):\n",
    "            raise ValueError(f\"Input file {input_file} does not exist!\")\n",
    "        if len(input_files) == 1:\n",
    "            output_file_base = \"predictions.jsonl\"\n",
    "        else:\n",
    "            input_file_base = os.path.basename(input_file)\n",
    "            output_file_base = f\"predictions-{input_file_base}\"\n",
    "        output_file = os.path.join(args.output_dir, output_file_base)\n",
    "        predictions = []\n",
    "        xpredictions = []\n",
    "\n",
    "        with open(input_file, \"r\") as eval_file, open(output_file, \"w\") as preds_file:\n",
    "            batch_instances = []\n",
    "            xbatch_instances = []\n",
    "            counter = 0\n",
    "            for line in eval_file:\n",
    "                if args.n_eval > 0 and counter >= args.n_eval:\n",
    "                    break\n",
    "                try:\n",
    "                    instance_raw = json.loads(line.strip())\n",
    "                except:\n",
    "                    instance_raw = {\"input\": line.strip()}\n",
    "                counter += 1\n",
    "                if counter % 100 == 0:\n",
    "                    logger.info(f\"Processed {counter} lines.\")\n",
    "                angle = instance_raw.get('angle')\n",
    "                explicit_outputs = instance_raw.get('explicit_outputs')\n",
    "                instance = {'id': instance_raw.get('id', f'line-{counter}')}\n",
    "                instance['line'] = counter\n",
    "                if angle is None:\n",
    "                    instance['input'] = instance_raw['input']\n",
    "                else:\n",
    "                    if len(angle) != 2:\n",
    "                        raise ValueError(f\"Invalid angle: {angle}\")\n",
    "                    instance['angle'] = angle\n",
    "                    slots = {k:v for k,v in instance_raw.items() if k in angle[0] or k in angle[1]}\n",
    "                    instance['slots'] = slots\n",
    "                    input, _, angle_str = make_input_string(slots, angle)\n",
    "                    instance['input'] = input\n",
    "                    instance['angle_str'] = angle_str\n",
    "                    if explicit_outputs is not None:\n",
    "                        assert len(angle[1]) == 1\n",
    "                        slot = angle[1][0]\n",
    "                        for explicit_output in explicit_outputs:\n",
    "                            xinstance = instance.copy()\n",
    "                            output_raw = make_input_string({slot: explicit_output})[0]\n",
    "                            xinstance['output'] = output_raw\n",
    "                            xinstance['output_text'] = explicit_output\n",
    "                            xbatch_instances.append(xinstance)\n",
    "                            if len(xbatch_instances) == args.eval_batch_size:\n",
    "                                xpredictions += run_model_with_outputs(args, model, tokenizer, xbatch_instances)\n",
    "                                xbatch_instances = []\n",
    "                batch_instances.append(instance)\n",
    "\n",
    "                if len(batch_instances) == args.eval_batch_size:\n",
    "                    predictions += run_generate(args, model, tokenizer, batch_instances)\n",
    "                    # Show the first set of predictions\n",
    "                    if len(predictions) == args.eval_batch_size:\n",
    "                        for p in predictions:\n",
    "                            logger.info(f\"Sample prediction: {p}\")\n",
    "\n",
    "                    batch_instances = []\n",
    "            if len(batch_instances) > 0:\n",
    "                predictions += run_generate(args, model, tokenizer, batch_instances)\n",
    "            if len(xbatch_instances) > 0:\n",
    "                xpredictions += run_model_with_outputs(args, model, tokenizer, xbatch_instances)\n",
    "            # collate instances using explicit_outputs\n",
    "            xpredictions_idx = 0\n",
    "            for prediction in predictions:\n",
    "                explicit_output_res = []\n",
    "                current_line = prediction['line']\n",
    "                if args.add_metrics:\n",
    "                    pred_slots = prediction['output_slots_list'][0]\n",
    "                    gold_slots = prediction.get('slots', {})\n",
    "                    if len(set(pred_slots.keys()).intersection(gold_slots.keys())) > 0:\n",
    "                        metrics = score_prediction({\"slots\": pred_slots, \"angle\": prediction['angle']}, gold_slots)\n",
    "                        prediction['metrics'] = metrics\n",
    "                while xpredictions_idx < len(xpredictions) and xpredictions[xpredictions_idx]['line'] == current_line:\n",
    "                    explicit_output_res.append(xpredictions[xpredictions_idx])\n",
    "                    xpredictions_idx += 1\n",
    "                if len(explicit_output_res) > 0:\n",
    "                    explicit_output_res.sort(key = lambda x: -x['prediction']['output_prob'])\n",
    "                    prediction[\"explicit_outputs\"] = [x['prediction'] for x in explicit_output_res]\n",
    "                preds_file.write(json.dumps(prediction) + \"\\n\")\n",
    "            preds_file.flush()\n",
    "            if args.add_metrics:\n",
    "                collated = collate_scores(predictions)\n",
    "                all_metrics_aggregated[input_file] = collated['metrics_aggregated']\n",
    "    if args.add_metrics:\n",
    "        save_json(os.path.join(args.output_dir, \"metrics.json\"), all_metrics_aggregated)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a427daaa2819ce086300a38f7f2f087a1c29849f8455e213bfda40f0b94a2c0d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('nluintel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
